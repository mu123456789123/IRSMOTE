{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from fractions import Fraction\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve,auc\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import kde\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "inf = 9999999.0\n",
    "def loadDataSet(fileName, splitChar='\\t'):\n",
    "    dataSet = []\n",
    "    with open(fileName) as fr:\n",
    "         for line in fr.readlines():\n",
    "            curline = line.strip().split(splitChar)     \n",
    "            fltline = list(map(float, curline))\n",
    "            dataSet.append(fltline)\n",
    "    return dataSet\n",
    "\n",
    "\n",
    "def distEclud(vecA, vecB):\n",
    "    return np.sqrt(np.sum(np.power(vecA - vecB, 2))) \n",
    "\n",
    "\n",
    "def min_randCent(dataSet, k2):\n",
    "    dataSet = dataSet[np.nonzero(dataSet[:, -1].A == 0)[0]]\n",
    "    n =dataSet.shape[1]\n",
    "    mincentroids = np.mat(np.zeros((k2,n)))  \n",
    "    for j in range(n):\n",
    "        minJ = min(dataSet[:,j])\n",
    "        maxJ = max(dataSet[:,j])\n",
    "        rangeJ = float(maxJ - minJ)\n",
    "        mincentroids[:,j] = minJ + rangeJ * np.random.rand(k2, 1)\n",
    "    return mincentroids\n",
    "\n",
    "\n",
    "def min_kMeans(dataSet, k2, distMeans =distEclud, createCent = min_randCent):\n",
    "\n",
    "    dataSet = dataSet[np.nonzero(dataSet[:, -1].A == 0)[0]]\n",
    "    m = dataSet.shape[0]   \n",
    "    minclusterAssment = np.mat(np.zeros((m,2)))\n",
    "    mincentroids = createCent(dataSet, k2)\n",
    "    clusterChanged = True  \n",
    "    while clusterChanged:\n",
    "        clusterChanged = False;\n",
    "        for i in range(m): \n",
    "            minDist = inf; minIndex = -1;\n",
    "            for j in range(k2):\n",
    "                distJI = distMeans(mincentroids[j,:], dataSet[i,:])\n",
    "                if distJI < minDist:\n",
    "                    minDist = distJI; minIndex = j  \n",
    "            if minclusterAssment[i,0] != minIndex:\n",
    "                clusterChanged = True  \n",
    "            minclusterAssment[i,:] = minIndex,minDist**2   \n",
    "        for cent in range(k2):   \n",
    "            ptsInClust = dataSet[np.nonzero(minclusterAssment[:,0].A == cent)[0]]   \n",
    "            mincentroids[cent,:] = np.mean(ptsInClust, axis = 0)  \n",
    "    return mincentroids, minclusterAssment\n",
    "\n",
    "def maj_randCent(dataSet, k1):\n",
    "    dataSet = dataSet[np.nonzero(dataSet[:, -1].A == 1)[0]]\n",
    "    n = dataSet.shape[1]\n",
    "    majcentroids = np.mat(np.zeros((k1,n)))   \n",
    "    for j in range(n):\n",
    "        minJ = min(dataSet[:,j])\n",
    "        maxJ = max(dataSet[:,j])\n",
    "        rangeJ = float(maxJ - minJ)\n",
    "        majcentroids[:,j] = minJ + rangeJ * np.random.rand(k1, 1)\n",
    "    return majcentroids\n",
    "\n",
    "\n",
    "def maj_kMeans(dataSet, k1, distMeans =distEclud, createCent = maj_randCent):\n",
    "\n",
    "    dataSet = dataSet[np.nonzero(dataSet[:, -1].A == 1)[0]]\n",
    "    m = dataSet.shape[0]   \n",
    "    majclusterAssment = np.mat(np.zeros((m,2)))\n",
    "    majcentroids = createCent(dataSet, k1)\n",
    "    clusterChanged = True   \n",
    "    while clusterChanged:\n",
    "        clusterChanged = False;\n",
    "        for i in range(m):  \n",
    "            minDist = inf; minIndex = -1;\n",
    "            for j in range(k1):\n",
    "                distJI = distMeans(majcentroids[j,:], dataSet[i,:])\n",
    "                if distJI < minDist:\n",
    "                    minDist = distJI; minIndex = j  \n",
    "            if majclusterAssment[i,0] != minIndex:\n",
    "                clusterChanged = True  \n",
    "            majclusterAssment[i,:] = minIndex,minDist**2   \n",
    "        for cent in range(k1):   \n",
    "            ptsInClust = dataSet[np.nonzero(majclusterAssment[:,0].A == cent)[0]]   \n",
    "            majcentroids[cent,:] = np.mean(ptsInClust, axis = 0) \n",
    "    return majcentroids, majclusterAssment\n",
    "\n",
    "def irsmote(dataSet,k1,k2,p):\n",
    "    X=dataSet[:,0:-1]\n",
    "    y=dataSet[:,-1]\n",
    "    majcentroids2, majclustAssing = maj_kMeans(dataSet,k1)\n",
    "    mincentroids2, minclustAssing = min_kMeans(dataSet,k2)\n",
    "    majcentroids2=np.mat(majcentroids2)\n",
    "    majcentroids1 = majcentroids2[:,0:-1]                                 \n",
    "    minsamples1 = X[np.nonzero(dataSet[:, -1].A == 0)[0]]\n",
    "    s=minsamples1.shape[0]\n",
    "    n=majcentroids1.shape[0]\n",
    "    dist=np.zeros(shape=(n,s))\n",
    "    for i in range(n):\n",
    "        for j in range(s):\n",
    "            dist[i,j] = distEclud(majcentroids1[i, :], minsamples1[j, :])             \n",
    "    dist=dist.reshape(n*s,order='C')\n",
    "    dist=np.sort(dist)\n",
    "    mean_dist=np.mean(dist[0:p])\n",
    "    min_dist1=np.mat(minclustAssing)\n",
    "    min_dist2=min_dist1[:,-1]\n",
    "    noise=[]\n",
    "    clustAssing2=[]\n",
    "    for min_dist in  min_dist2:\n",
    "        if min_dist>mean_dist:\n",
    "            noise=dataSet[np.nonzero(minclustAssing[:, -1].A == min_dist)[0]]\n",
    "            clustAssing1=minclustAssing[np.nonzero(minclustAssing[:, -1].A == min_dist)[0]]  \n",
    "    nn = NearestNeighbors(n_neighbors=5, n_jobs=1)\n",
    "    nn.fit(X)\n",
    "    distances, indices = nn.kneighbors(X) \n",
    "    to_remove = []\n",
    "    dataSet2=dataSet\n",
    "    for i in range(len(noise)):\n",
    "        if (y[(np.where((dataSet==noise[i]).all(axis=1))[0])]  == 1 and\n",
    "            np.sum(y[(np.where((dataSet==noise[i]).all(axis=1))[0])] ==y[indices[(np.where((dataSet==noise[i]).all(axis=1))[0])][1:]])==0):\n",
    "            to_remove.append(i)\n",
    "        elif (y[(np.where((dataSet==noise[i]).all(axis=1))[0])]  == 0 and\n",
    "            np.sum(y[(np.where((dataSet==noise[i]).all(axis=1))[0])] ==y[indices[(np.where((dataSet==noise[i]).all(axis=1))[0])][1:]])==0):\n",
    "            to_remove.append(i)\n",
    "    to_remove = list(set(to_remove))\n",
    "    noise1=np.delete(noise, to_remove, axis=0)\n",
    "    dataSet1=np.delete(dataSet,noise1,axis=0)    \n",
    "    X1=dataSet1[:,0:-1] \n",
    "    y1=dataSet1[:,-1]\n",
    "    minsamples2 = X1[np.nonzero(dataSet1[:, -1].A == 0)[0]]\n",
    "    majsamples = X1[np.nonzero(dataSet1[:, -1].A == 1)[0]]\n",
    "    minsamples_count=minsamples2.shape[0]\n",
    "    majsamples_count=majsamples.shape[0]\n",
    "    synthetic_count=majsamples_count-minsamples_count\n",
    "    sparsity_factors=(np.zeros((k2,1),dtype=np.float64))\n",
    "    sparsity_sum = 0\n",
    "    imbalance_ratio_threshold = 0.9\n",
    "    for i in range(k2):    \n",
    "        mincentroids3=mincentroids2[i,:]\n",
    "        mincentroids4=mincentroids3[:,0:-1]\n",
    "        minsamples3= dataSet1[np.nonzero(minclustAssing[:, 0].A == i)[0]]\n",
    "        minsamples4=minsamples3[:,0:-1]\n",
    "        c=minsamples4.shape[0]\n",
    "        bandwidth1=((4/(3*c))**(1/5))*np.std(minsamples4)\n",
    "        kde1= kde.KernelDensity(kernel='gaussian', bandwidth=bandwidth1).fit(minsamples4)\n",
    "        density_factor1=kde1.score_samples(mincentroids4.reshape(1, -1))\n",
    "        density_factor=np.exp(density_factor1)\n",
    "        sparsity_factors[i] = 1 / density_factor\n",
    "    sparsity_sum = sparsity_factors.sum()\n",
    "    if sparsity_sum == 0:\n",
    "            sparsity_sum = 1 # to avoid division by zero\n",
    "            sparsity_sum = np.full(sparsity_factors.shape, sparsity_sum, np.asarray(sparsity_sum).dtype)  #生成一个i行1列的矩阵\n",
    "    sampling_weights = (sparsity_factors / sparsity_sum)\n",
    "    cluster_synthetic_count=synthetic_count*sampling_weights\n",
    "    a = {i:2*i for i in range(k2)}\n",
    "    for i in a:     #遍历聚类标签\n",
    "        synthetic_sample1=np.zeros((int(cluster_synthetic_count[i]), dataSet.shape[1]-1))\n",
    "        synthetic_sample2=np.zeros((int(cluster_synthetic_count[i]), dataSet.shape[1]))\n",
    "        tem_sample=np.zeros((1, dataSet.shape[1]-1))\n",
    "        minsamples=dataSet1[np.nonzero(minclustAssing[:, 0].A == i)[0]]    \n",
    "        if minsamples.shape[0]>3:\n",
    "            b = np.random.randint(0,minsamples.shape[0],2)\n",
    "            x=minsamples[b[0]]\n",
    "            y=minsamples[b[1]]\n",
    "            for k in range(int(cluster_synthetic_count[i])):\n",
    "                for j in range(minsamples.shape[1]-1):\n",
    "                    dif1=y[0,j]-x[0,j]\n",
    "                    gap=random.random()\n",
    "                    tem_sample[0,j]=x[0,j]+gap*dif1\n",
    "                    dif2=mincentroids4[0,j]-tem_sample[0,j]\n",
    "                    synthetic_sample1[k,j]=tem_sample[0,j]+gap*dif2\n",
    "                    synthetic_sample2=np.append(synthetic_sample1,np.zeros((synthetic_sample1.shape[0],1)), axis=1)\n",
    "        a[i]=synthetic_sample2\n",
    "        DataSet=np.array(dataSet1)\n",
    "    for k2,synthetic_sample2 in a.items():              \n",
    "        DataSet=np.append(DataSet,a[k2],axis=0)          \n",
    "    return DataSet    \n",
    " \n",
    "def main():\n",
    "    dataSet = loadDataSet(r'C:\\Users\\jupyter\\数据集\\german.txt', splitChar=' ')\n",
    "    dataSet = np.mat(dataSet)\n",
    "    UPPER_BOUND, LOWER_BOUND = 0.9, 0.1\n",
    "    data_x = np.array(dataSet[:, :-1], dtype=np.float32)  \n",
    "    data_y = np.array(dataSet[:, -1], dtype=np.int32)  \n",
    "    x_min = np.min(data_x, axis=0)  \n",
    "    x_max = np.max(data_x, axis=0)  \n",
    "    data_x = (data_x - x_min) / x_max * (UPPER_BOUND - LOWER_BOUND) + LOWER_BOUND   \n",
    "    data_num = data_x.shape[0]\n",
    "    feature_num = data_x.shape[1]\n",
    "    rnd_index = np.arange(data_num)\n",
    "    np.random.shuffle(rnd_index)  \n",
    "    data_x = data_x[rnd_index]\n",
    "    data_y = data_y[rnd_index]\n",
    "    dataSet=np.append(data_x,data_y,axis=1)\n",
    "    dataSet = np.mat(dataSet)\n",
    "    X=dataSet[:,0:-1]\n",
    "    y=dataSet[:,-1] \n",
    "    DataSet =irsmote(dataSet,k1=2,k2=2,p=70)\n",
    "    print(DataSet)\n",
    "    print(DataSet.shape)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    start = time.clock()\n",
    "    main()\n",
    "    end = time.clock()\n",
    "    print('finish all in %s' % str(end - start))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
